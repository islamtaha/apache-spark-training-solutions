import org.apache.spark._

object Main {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("test").setMaster("local[*]")
    val sc = new SparkContext(conf)

  	  val dd = sc.textFile("/home/islam/spark-training-master/examples/pairs/wordcount/input")
  val rdd1 = dd.flatMap { x => x.split(" ") } 

  
  val rdd2 = rdd1.map { _ -> 1 }
  
  val output = rdd2.reduceByKey(_ + _)
  
  val out = output.collect()
  }
}
